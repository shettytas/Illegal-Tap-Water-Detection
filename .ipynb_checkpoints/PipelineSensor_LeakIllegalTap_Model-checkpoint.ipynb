{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9720797-3f1c-49ba-9c14-872a0a7b52c6",
   "metadata": {},
   "source": [
    "# Illegal Tap Water Detection Using Pipeline Sensor Data\n",
    "To build a machine learning model that detects water theft through illegal pipeline taps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b84b71-d2e6-411f-8d4c-d55ae49d2670",
   "metadata": {},
   "source": [
    "## Overview of Dataset\n",
    "This section gives a brief overview of the dataset, including its columns and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e6360e-72d6-471e-b76f-4ade302d704d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>SensorID</th>\n",
       "      <th>pressure</th>\n",
       "      <th>flow_rate</th>\n",
       "      <th>leak_intensity</th>\n",
       "      <th>illegal_tap_flag</th>\n",
       "      <th>event_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1:00 PM</td>\n",
       "      <td>S001</td>\n",
       "      <td>2.82</td>\n",
       "      <td>123.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1:10 PM</td>\n",
       "      <td>S001</td>\n",
       "      <td>2.71</td>\n",
       "      <td>113.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1:20 PM</td>\n",
       "      <td>S001</td>\n",
       "      <td>2.84</td>\n",
       "      <td>121.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1:30 PM</td>\n",
       "      <td>S001</td>\n",
       "      <td>2.82</td>\n",
       "      <td>121.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1:40 PM</td>\n",
       "      <td>S001</td>\n",
       "      <td>2.75</td>\n",
       "      <td>121.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time SensorID  pressure  flow_rate  leak_intensity  illegal_tap_flag  \\\n",
       "0  1:00 PM     S001      2.82     123.91             0.0                 0   \n",
       "1  1:10 PM     S001      2.71     113.40             0.0                 0   \n",
       "2  1:20 PM     S001      2.84     121.48             0.0                 0   \n",
       "3  1:30 PM     S001      2.82     121.73             0.0                 0   \n",
       "4  1:40 PM     S001      2.75     121.16             0.0                 0   \n",
       "\n",
       "  event_type  \n",
       "0     normal  \n",
       "1     normal  \n",
       "2     normal  \n",
       "3     normal  \n",
       "4     normal  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"pipeline_sensor_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b393c08-1924-46c2-877e-2eccb00dfa6c",
   "metadata": {},
   "source": [
    "## Handling Categorical Variables: Columns like “SensorID”, “Time” and \"event_type\" need to be encoded numerically\n",
    "Loads the sensor dataset, identifies categorical columns (`SensorID`, `Time`, and `event_type`), and encodes them into numeric values using pandas’ built-in category codes. The encoded data is then saved as a new CSV file (`encoded_pipeline_sensor_data.csv`) for further processing or machine learning.\n",
    "\n",
    "* normal - 5\n",
    "* low_leak - 3\n",
    "* blockage - 0\n",
    "* leak_high - 2\n",
    "* irregular_flow - 1\n",
    "* negative_pressure - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca33f06a-4bec-4455-bdfb-8123a79ad14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time                 object\n",
      "SensorID             object\n",
      "pressure            float64\n",
      "flow_rate           float64\n",
      "leak_intensity      float64\n",
      "illegal_tap_flag      int64\n",
      "event_type           object\n",
      "dtype: object\n",
      "  SensorID  SensorID_encoded\n",
      "0     S001                 0\n",
      "1     S001                 0\n",
      "2     S001                 0\n",
      "3     S001                 0\n",
      "4     S001                 0\n",
      "  event_type  event_type_encoded\n",
      "0     normal                   5\n",
      "1     normal                   5\n",
      "2     normal                   5\n",
      "3     normal                   5\n",
      "4     normal                   5\n",
      "      Time  Time_encoded\n",
      "0  1:00 PM             0\n",
      "1  1:10 PM             1\n",
      "2  1:20 PM             2\n",
      "3  1:30 PM             3\n",
      "4  1:40 PM             4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>SensorID</th>\n",
       "      <th>pressure</th>\n",
       "      <th>flow_rate</th>\n",
       "      <th>leak_intensity</th>\n",
       "      <th>illegal_tap_flag</th>\n",
       "      <th>event_type</th>\n",
       "      <th>SensorID_encoded</th>\n",
       "      <th>event_type_encoded</th>\n",
       "      <th>Time_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1:00 PM</td>\n",
       "      <td>S001</td>\n",
       "      <td>2.82</td>\n",
       "      <td>123.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1:10 PM</td>\n",
       "      <td>S001</td>\n",
       "      <td>2.71</td>\n",
       "      <td>113.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1:20 PM</td>\n",
       "      <td>S001</td>\n",
       "      <td>2.84</td>\n",
       "      <td>121.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1:30 PM</td>\n",
       "      <td>S001</td>\n",
       "      <td>2.82</td>\n",
       "      <td>121.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1:40 PM</td>\n",
       "      <td>S001</td>\n",
       "      <td>2.75</td>\n",
       "      <td>121.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time SensorID  pressure  flow_rate  leak_intensity  illegal_tap_flag  \\\n",
       "0  1:00 PM     S001      2.82     123.91             0.0                 0   \n",
       "1  1:10 PM     S001      2.71     113.40             0.0                 0   \n",
       "2  1:20 PM     S001      2.84     121.48             0.0                 0   \n",
       "3  1:30 PM     S001      2.82     121.73             0.0                 0   \n",
       "4  1:40 PM     S001      2.75     121.16             0.0                 0   \n",
       "\n",
       "  event_type  SensorID_encoded  event_type_encoded  Time_encoded  \n",
       "0     normal                 0                   5             0  \n",
       "1     normal                 0                   5             1  \n",
       "2     normal                 0                   5             2  \n",
       "3     normal                 0                   5             3  \n",
       "4     normal                 0                   5             4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('pipeline_sensor_data.csv')\n",
    "print(df.dtypes)\n",
    "\n",
    "df['SensorID_encoded'] = df['SensorID'].astype('category').cat.codes\n",
    "print(df[['SensorID', 'SensorID_encoded']].head())\n",
    "\n",
    "df['event_type_encoded'] = df['event_type'].astype('category').cat.codes\n",
    "print(df[['event_type', 'event_type_encoded']].head())\n",
    "\n",
    "df['Time_encoded'] = df['Time'].astype('category').cat.codes\n",
    "print(df[['Time', 'Time_encoded']].head())\n",
    "\n",
    "df.to_csv('encoded_pipeline_sensor_data.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ac26f-b458-4657-a067-fdc471002edd",
   "metadata": {},
   "source": [
    "## Splitting Dataset by Sensor Segments for Train and Test\n",
    "The dataset is split into training and testing sets based on sensor arguements. We are randomly choosing 80% of them for training and 20% for testing. This helps make sure the model is tested on new sensors it hasn’t seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07bff6d1-26f4-4cf6-b72f-57ff8d4172eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train segments: 160, data points: 8000\n",
      "Test segments: 40, data points: 2000\n",
      "Train class distribution:\n",
      "event_type_encoded\n",
      "1    0.012125\n",
      "2    0.017250\n",
      "3    0.008250\n",
      "4    0.006125\n",
      "5    0.956250\n",
      "Name: proportion, dtype: float64\n",
      "Test class distribution:\n",
      "event_type_encoded\n",
      "0    0.1420\n",
      "1    0.1820\n",
      "2    0.1095\n",
      "3    0.0995\n",
      "4    0.0230\n",
      "5    0.4440\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('encoded_pipeline_sensor_data.csv')\n",
    "\n",
    "# Get unique segments\n",
    "unique_segments = df['SensorID'].unique()\n",
    "\n",
    "# Randomly split segments (without stratification)\n",
    "train_segments, test_segments = train_test_split(\n",
    "    unique_segments,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Select full segments for train and test datasets\n",
    "train_df = df[df['SensorID'].isin(train_segments)]\n",
    "test_df = df[df['SensorID'].isin(test_segments)]\n",
    "\n",
    "# Display counts and class distribution\n",
    "print(f'Train segments: {len(train_segments)}, data points: {len(train_df)}')\n",
    "print(f'Test segments: {len(test_segments)}, data points: {len(test_df)}')\n",
    "\n",
    "print('Train class distribution:')\n",
    "print(train_df['event_type_encoded'].value_counts(normalize=True).sort_index())\n",
    "\n",
    "print('Test class distribution:')\n",
    "print(test_df['event_type_encoded'].value_counts(normalize=True).sort_index())\n",
    "\n",
    "train_df.to_csv('train_split.csv', index=False)\n",
    "test_df.to_csv('test_split.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c893e-f9f7-4ba1-90d5-0c4babdafdc2",
   "metadata": {},
   "source": [
    "# Past-based features\n",
    "We make new features from the past readings of each sensor. Instead of looking at just the current pressure and flow rate, we also add information about how these values changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab459c63-4f42-4bc6-ba2d-735d3a5e385e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4400, 30) Test: (1600, 30)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ID_COL = 'SensorID_encoded'\n",
    "TIME_COL = 'Time_encoded'\n",
    "SIG_COLS = ['pressure', 'flow_rate']\n",
    "LAGS = [1, 3, 5, 10]\n",
    "ROLL_WINDOWS = [5, 10, 20]\n",
    "EPS = 1e-6\n",
    "\n",
    "def rolling_slope_past_only(values: np.ndarray, window: int) -> np.ndarray:\n",
    "    n = len(values)\n",
    "    out = np.full(n, np.nan)\n",
    "    t = np.arange(window, dtype=float)\n",
    "    t_mean = t.mean()\n",
    "    var_t = ((t - t_mean) ** 2).sum()\n",
    "    if var_t == 0:\n",
    "        return out\n",
    "    for i in range(window - 1, n):\n",
    "        w = values[i - window + 1 : i + 1]\n",
    "        if np.isnan(w).any():\n",
    "            continue\n",
    "        x_mean = w.mean()\n",
    "        cov_tx = ((t - t_mean) * (w - x_mean)).sum()\n",
    "        out[i] = cov_tx / var_t\n",
    "    return out\n",
    "\n",
    "def add_past_only_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.sort_values([ID_COL, TIME_COL]).copy()\n",
    "    g = df.groupby(ID_COL, group_keys=False)\n",
    "\n",
    "    # Lags\n",
    "    for s in SIG_COLS:\n",
    "        for L in LAGS:\n",
    "            df[f'{s}_lag{L}'] = g[s].shift(L)\n",
    "\n",
    "    # Delta and percent change from lag1\n",
    "    for s in SIG_COLS:\n",
    "        df[f'{s}_delta1'] = df[s] - df[f'{s}_lag1']\n",
    "        df[f'{s}_pct1'] = df[f'{s}_delta1'] / (np.abs(df[f'{s}_lag1']) + EPS)\n",
    "\n",
    "    # Rolling stats (past-only)\n",
    "    for s in SIG_COLS:\n",
    "        s_past = g[s].shift(1)\n",
    "        for w in ROLL_WINDOWS:\n",
    "            roll = s_past.groupby(df[ID_COL]).rolling(window=w, min_periods=w)\n",
    "            df[f'{s}_roll{w}_mean'] = roll.mean().reset_index(level=0, drop=True)\n",
    "            df[f'{s}_roll{w}_std']  = roll.std().reset_index(level=0, drop=True)\n",
    "            df[f'{s}_roll{w}_min']  = roll.min().reset_index(level=0, drop=True)\n",
    "            df[f'{s}_roll{w}_max']  = roll.max().reset_index(level=0, drop=True)\n",
    "            df[f'{s}_roll{w}_med']  = roll.median().reset_index(level=0, drop=True)\n",
    "\n",
    "    # Rolling slopes (past-only)\n",
    "    for s in SIG_COLS:\n",
    "        s_past = g[s].shift(1)\n",
    "        arr = s_past.to_numpy()\n",
    "        for w in ROLL_WINDOWS:\n",
    "            col = f'{s}_roll{w}_slope'\n",
    "            df[col] = np.nan\n",
    "            for _, idx in g.indices.items():\n",
    "                idx = np.array(idx)\n",
    "                sl = rolling_slope_past_only(arr[idx], w)\n",
    "                df.loc[idx, col] = sl\n",
    "\n",
    "    # Simple interaction\n",
    "    df['ratio_flow_pressure'] = df['flow_rate_lag1'] / (np.abs(df['pressure_lag1']) + EPS)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Build features on the entire df first\n",
    "df_feat = add_past_only_features(df)\n",
    "\n",
    "# Drop rows without enough history\n",
    "df_feat = df_feat.dropna().reset_index(drop=True)\n",
    "\n",
    "# Recreate the same time-aware mask on df_feat\n",
    "df_feat_sorted = df_feat.sort_values([ID_COL, TIME_COL]).reset_index(drop=True)\n",
    "mask_feat = df_feat_sorted.groupby(ID_COL, group_keys=False).apply(\n",
    "    lambda g: pd.Series([True]*int(len(g)*0.75) + [False]*(len(g)-int(len(g)*0.75)), index=g.index)\n",
    ")\n",
    "\n",
    "# Define features to use in X\n",
    "FEATURE_COLS = (\n",
    "    ['pressure','flow_rate','leak_intensity','illegal_tap_flag','SensorID_encoded','ratio_flow_pressure'] +\n",
    "    [f'pressure_lag{k}' for k in LAGS] + [f'flow_rate_lag{k}' for k in LAGS] +\n",
    "    ['pressure_delta1','pressure_pct1','flow_rate_delta1','flow_rate_pct1'] +\n",
    "    ['pressure_roll5_mean','pressure_roll10_std','pressure_roll20_min',\n",
    "     'flow_rate_roll5_mean','flow_rate_roll10_std','flow_rate_roll20_min',\n",
    "     'pressure_roll5_slope','pressure_roll10_slope','pressure_roll20_slope',\n",
    "     'flow_rate_roll5_slope','flow_rate_roll10_slope','flow_rate_roll20_slope']\n",
    ")\n",
    "\n",
    "X_all = df_feat_sorted[FEATURE_COLS].copy()\n",
    "y_all = df_feat_sorted['event_type_encoded'].copy()\n",
    "\n",
    "X_train, X_test = X_all[mask_feat.values], X_all[~mask_feat.values]\n",
    "y_train, y_test = y_all[mask_feat.values], y_all[~mask_feat.values]\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952971fb-51ae-436e-a2e4-751d6b5e7d7c",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "We train an XGBoost model to predict the type of pipeline event (for example, normal flow, leakage, or illegal tap) using the sensor data. We read the pre-split training and testing datasets and then use key sensor readings (`pressure`, `flow_rate`, `leak_intensity`, etc.) to predict the column `event_type_encoded`. We also handle class imbalance by giving rare classes more importance during training. The classification report (precision, recall, F1-score). The confusion matrix, which shows how often each class was confused with others. Finally, we map the numeric predictions back to their original event type names to make the results easier to understand.\n",
    "\n",
    "This gives us a clear view of how well the model can detect different types of pipeline events from the sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "689bac5c-fc85-48c2-9703-2df88381ef5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_type\n",
      "normal               7650\n",
      "leak_high             138\n",
      "irregular_flow         97\n",
      "leak_low               66\n",
      "negative_pressure      49\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train_split.csv')\n",
    "print(train['event_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4771831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: event_type\n",
      "normal               7650\n",
      "leak_high             138\n",
      "irregular_flow         97\n",
      "leak_low               66\n",
      "negative_pressure      49\n",
      "Name: count, dtype: int64\n",
      "TEST: event_type\n",
      "normal               888\n",
      "irregular_flow       364\n",
      "blockage             284\n",
      "leak_high            219\n",
      "leak_low             199\n",
      "negative_pressure     46\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train_split.csv')\n",
    "test = pd.read_csv('test_split.csv')\n",
    "\n",
    "print(\"TRAIN:\", train['event_type'].value_counts())\n",
    "print(\"TEST:\", test['event_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12124f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Time SensorID  pressure  flow_rate  leak_intensity  illegal_tap_flag  \\\n",
      "129   5:50 PM     S017      7.25       5.07             0.0                 1   \n",
      "130   6:00 PM     S017      7.32       5.22             0.0                 1   \n",
      "131   6:10 PM     S017      7.24       6.43             0.0                 1   \n",
      "132   6:20 PM     S017      7.25       5.04             0.0                 1   \n",
      "133   6:30 PM     S017      7.18       6.87             0.0                 1   \n",
      "...       ...      ...       ...        ...             ...               ...   \n",
      "1795  8:30 PM     S171      7.36       5.17             0.0                 1   \n",
      "1796  8:40 PM     S171      6.68       6.39             0.0                 1   \n",
      "1797  8:50 PM     S171      6.98       6.21             0.0                 1   \n",
      "1798  9:00 PM     S171      6.66       4.83             0.0                 1   \n",
      "1799  9:10 PM     S171      7.07       5.39             0.0                 1   \n",
      "\n",
      "     event_type  SensorID_encoded  event_type_encoded  Time_encoded  \n",
      "129    blockage                16                   0            29  \n",
      "130    blockage                16                   0            30  \n",
      "131    blockage                16                   0            31  \n",
      "132    blockage                16                   0            32  \n",
      "133    blockage                16                   0            33  \n",
      "...         ...               ...                 ...           ...  \n",
      "1795   blockage               170                   0            45  \n",
      "1796   blockage               170                   0            46  \n",
      "1797   blockage               170                   0            47  \n",
      "1798   blockage               170                   0            48  \n",
      "1799   blockage               170                   0            49  \n",
      "\n",
      "[284 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "test_blockage = test[test['event_type'] == 'blockage']\n",
    "print(test_blockage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56cfa866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your existing files\n",
    "train = pd.read_csv('train_split.csv')\n",
    "test_split = pd.read_csv('test_split.csv')\n",
    "\n",
    "# Select blockage rows from test set\n",
    "test_blockage = test_split[test_split['event_type'] == 'blockage']\n",
    "\n",
    "# Randomly sample 70 blockage rows\n",
    "blockage_sample = test_blockage.sample(n=70, random_state=42)\n",
    "\n",
    "# Remove these rows from test set (so you don't duplicate in test)\n",
    "test_split = test_split.drop(blockage_sample.index)\n",
    "\n",
    "# Add them to the train set\n",
    "train = pd.concat([train, blockage_sample])\n",
    "\n",
    "# Shuffle train set to mix new rows well\n",
    "train = train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save updated files\n",
    "train.to_csv('train.csv', index=False)\n",
    "test_split.to_csv('test_split.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d4eb508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_type\n",
      "normal               7650\n",
      "leak_high             138\n",
      "irregular_flow         97\n",
      "blockage               70\n",
      "leak_low               66\n",
      "negative_pressure      49\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new train file\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# Show the number of each event type in train\n",
    "print(train['event_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f81b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv('test_split.csv')\n",
    "test_df = test_df.sample(frac=1, random_state=567).reset_index(drop=True)\n",
    "# rerun your evaluation using this shuffled test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d92842b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       214\n",
      "           1       1.00      0.97      0.98       364\n",
      "           2       1.00      1.00      1.00       219\n",
      "           3       1.00      1.00      1.00       199\n",
      "           4       0.96      1.00      0.98        46\n",
      "           5       1.00      1.00      1.00       888\n",
      "\n",
      "    accuracy                           0.99      1930\n",
      "   macro avg       0.99      0.99      0.99      1930\n",
      "weighted avg       0.99      0.99      0.99      1930\n",
      "\n",
      "[[213   1   0   0   0   0]\n",
      " [  8 354   0   0   2   0]\n",
      " [  0   0 219   0   0   0]\n",
      " [  0   0   0 199   0   0]\n",
      " [  0   0   0   0  46   0]\n",
      " [  0   0   0   0   0 888]]\n",
      "Actual : ['normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal']\n",
      "Predicted: ['normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. LOAD UPDATED TRAIN/TEST SPLIT FROM CSV\n",
    "train_df = pd.read_csv('train.csv')           # <--- changed here!\n",
    "test_df = pd.read_csv('test_split.csv')       # use your current test split\n",
    "\n",
    "# 2. DEFINE FEATURES & TARGET\n",
    "feature_cols = ['pressure', 'flow_rate', 'leak_intensity', 'illegal_tap_flag', 'SensorID_encoded', 'Time_encoded']\n",
    "target_col = 'event_type_encoded'\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "# 3. FIX LABEL ENCODING SO XGBOOST DOESN'T ERROR\n",
    "le = LabelEncoder()\n",
    "le.fit(list(y_train) + list(y_test))\n",
    "y_train_clean = le.transform(y_train)\n",
    "y_test_clean = le.transform(y_test)\n",
    "\n",
    "# 4. COMPUTE SAMPLE WEIGHTS FOR CLASS IMBALANCE\n",
    "classes = np.unique(y_train_clean)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y_train_clean)\n",
    "weight_dict = dict(zip(classes, class_weights))\n",
    "weights = pd.Series(y_train_clean).map(weight_dict)\n",
    "\n",
    "# 5. CREATE DMATRIX FOR XGBOOST\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train_clean, weight=weights)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test_clean)\n",
    "\n",
    "# 6. TRAIN XGBOOST MULTI-CLASS MODEL\n",
    "params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': len(le.classes_),\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'seed': 42\n",
    "}\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# 7. EVALUATE\n",
    "y_pred_prob = bst.predict(dtest)\n",
    "y_pred = y_pred_prob.argmax(axis=1)\n",
    "print(classification_report(y_test_clean, y_pred))\n",
    "print(confusion_matrix(y_test_clean, y_pred))\n",
    "\n",
    "# Create mapping from encoded labels to human-readable event types\n",
    "all_labels = pd.concat([\n",
    "    train_df[['event_type_encoded', 'event_type']],\n",
    "    test_df[['event_type_encoded', 'event_type']]\n",
    "]).drop_duplicates()\n",
    "\n",
    "mapping = dict(zip(all_labels['event_type_encoded'], all_labels['event_type']))\n",
    "\n",
    "actual_event_types = [mapping[x] for x in y_test_clean]\n",
    "predicted_event_types = [mapping[x] for x in y_pred]\n",
    "\n",
    "print(\"Actual :\", actual_event_types[:20])\n",
    "print(\"Predicted:\", predicted_event_types[:20])\n",
    "\n",
    "predicted_labels = le.inverse_transform(y_pred)\n",
    "actual_labels = le.inverse_transform(y_test_clean)\n",
    "\n",
    "# Save model\n",
    "bst.save_model(\"xgboost_pipeline_event_model.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
